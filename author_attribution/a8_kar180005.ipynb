{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Import packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from nltk.corpus import stopwords\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.preprocessing import StandardScaler"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Read CSV file. Display relevant data. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     author                                               text\n",
      "0  HAMILTON  FEDERALIST. No. 1 General Introduction For the...\n",
      "1       JAY  FEDERALIST No. 2 Concerning Dangers from Forei...\n",
      "2       JAY  FEDERALIST No. 3 The Same Subject Continued (C...\n",
      "3       JAY  FEDERALIST No. 4 The Same Subject Continued (C...\n",
      "4       JAY  FEDERALIST No. 5 The Same Subject Continued (C...\n",
      "\n",
      "author              \n",
      "HAMILTON                49\n",
      "MADISON                 15\n",
      "HAMILTON OR MADISON     11\n",
      "JAY                      5\n",
      "HAMILTON AND MADISON     3\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "df = pd.read_csv('./federalist.csv')\n",
    "# df[['author']] = df[['author']].astype('category')\n",
    "\n",
    "print(df.head()[:5])\n",
    "print()\n",
    "print(df[['author']].value_counts())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create training and testing datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training data shape: (66, 2)\n",
      "Testing data shape:  (17, 2)\n"
     ]
    }
   ],
   "source": [
    "# Split data into 80% train and 20% test sets\n",
    "# Use random state 1234 to ensure reproducibility\n",
    "train, test = train_test_split(df, test_size=0.2, random_state=1234)\n",
    "print('Training data shape:', train.shape)\n",
    "print('Testing data shape: ', test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Perform text preprocessing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set shape: (66, 7876)\n",
      "Testing set shape:  (17, 7876)\n"
     ]
    }
   ],
   "source": [
    "# Remove stop words from the datasets using tfifdfVectorizer\n",
    "stop_words = stopwords.words('english')\n",
    "vectorizer = TfidfVectorizer(stop_words=stop_words)\n",
    "\n",
    "# Fit and transform the vectorizer on the training data\n",
    "X_train = vectorizer.fit_transform(train['text'])\n",
    "y_train = train['author']\n",
    "\n",
    "# Transform the vectorizer on the test data\n",
    "X_test = vectorizer.transform(test['text'])\n",
    "y_test = test['author']\n",
    "\n",
    "print('Training set shape:', X_train.shape)\n",
    "print('Testing set shape: ', X_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Classify the data using Naive Bayes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 58.82%\n"
     ]
    }
   ],
   "source": [
    "# Fit the data to Naive Bayes model\n",
    "nb = MultinomialNB()\n",
    "nb.fit(X_train, y_train)\n",
    "\n",
    "# Get the accuracy score for the Naive Bayes model\n",
    "acc = nb.score(X_test, y_test)\n",
    "\n",
    "print('Accuracy:', str(round(acc * 100, 2)) + '%')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Why the low accuracy? The data is imbalanced and this causes the model to be biased towards the majority class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train set predictions: \n",
      " ['HAMILTON' 'HAMILTON' 'HAMILTON' 'HAMILTON' 'HAMILTON' 'HAMILTON'\n",
      " 'HAMILTON' 'HAMILTON' 'HAMILTON' 'HAMILTON' 'HAMILTON' 'HAMILTON'\n",
      " 'HAMILTON' 'HAMILTON' 'HAMILTON' 'HAMILTON' 'HAMILTON' 'HAMILTON'\n",
      " 'HAMILTON' 'HAMILTON' 'HAMILTON' 'HAMILTON' 'HAMILTON' 'HAMILTON'\n",
      " 'HAMILTON' 'HAMILTON' 'HAMILTON' 'HAMILTON' 'HAMILTON' 'HAMILTON'\n",
      " 'HAMILTON' 'HAMILTON' 'HAMILTON' 'HAMILTON' 'HAMILTON' 'HAMILTON'\n",
      " 'HAMILTON' 'HAMILTON' 'HAMILTON' 'HAMILTON' 'HAMILTON' 'HAMILTON'\n",
      " 'HAMILTON' 'HAMILTON' 'HAMILTON' 'HAMILTON' 'HAMILTON' 'HAMILTON'\n",
      " 'HAMILTON' 'HAMILTON' 'HAMILTON' 'HAMILTON' 'HAMILTON' 'HAMILTON'\n",
      " 'HAMILTON' 'HAMILTON' 'HAMILTON' 'HAMILTON' 'HAMILTON' 'HAMILTON'\n",
      " 'HAMILTON' 'HAMILTON' 'HAMILTON' 'HAMILTON' 'HAMILTON' 'HAMILTON']\n",
      "\n",
      "Test set predictions: \n",
      " ['HAMILTON' 'HAMILTON' 'HAMILTON' 'HAMILTON' 'HAMILTON' 'HAMILTON'\n",
      " 'HAMILTON' 'HAMILTON' 'HAMILTON' 'HAMILTON' 'HAMILTON' 'HAMILTON'\n",
      " 'HAMILTON' 'HAMILTON' 'HAMILTON' 'HAMILTON' 'HAMILTON']\n"
     ]
    }
   ],
   "source": [
    "pred = nb.predict(X_train)\n",
    "pred2 = nb.predict(X_test)\n",
    "\n",
    "print('Train set predictions: \\n', pred)\n",
    "print()\n",
    "print('Test set predictions: \\n', pred2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The accuracy of the Naive Bayes model is lower than expected so well try to improve it by changing the vectorizer parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 229,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 58.82%\n",
      "didnt change :(\n"
     ]
    }
   ],
   "source": [
    "# Limit the number of features to 1000\n",
    "# Utilize unigrams and bigrams\n",
    "vectorizer2 = TfidfVectorizer(stop_words=stop_words, max_features=1000, ngram_range=(1, 2))\n",
    "\n",
    "X_train2 = vectorizer2.fit_transform(train['text'])\n",
    "y_train2 = train['author']\n",
    "\n",
    "X_test2 = vectorizer2.transform(test['text'])\n",
    "y_test2 = test['author']\n",
    "\n",
    "nb2 = MultinomialNB()\n",
    "nb.fit(X_train2, y_train2)\n",
    "acc2 = nb.score(X_test2, y_test2)\n",
    "print('Accuracy:', str(round(acc2 * 100, 2)) + '%')\n",
    "print(\"didnt change :(\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Classify the data using Logistic Regression.\n",
    "- Changing the maximum iterations seemed to improve the results. (though the results are still lackluster)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 227,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 70.59%\n"
     ]
    }
   ],
   "source": [
    "weights = {'HAMILTON': 49, 'MADISON': 15, 'JAY': 5, 'HAMILTON OR MADISON': 11, 'HAMILTON AND MADISON': 3}\n",
    "lr = LogisticRegression(class_weight=weights, multi_class='multinomial', max_iter=1000)\n",
    "lr.fit(X_train, y_train)\n",
    "\n",
    "acc = lr.score(X_test, y_test)\n",
    "print('Accuracy:', str(round(acc * 100, 2)) + '%')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Classify the data using a Neural Network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 223,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 88.24%\n"
     ]
    }
   ],
   "source": [
    "# Scale the data for faster convergence\n",
    "scaler = StandardScaler(with_mean=False)\n",
    "scaler.fit(X_train2)\n",
    "X_train2_scaled = scaler.transform(X_train2)\n",
    "X_test2_scaled = scaler.transform(X_test2)\n",
    "\n",
    "# Fit the data to a neural network\n",
    "regr = MLPClassifier(random_state=1234, max_iter=1000, hidden_layer_sizes=(100))\n",
    "regr.fit(X_train2_scaled, y_train2)\n",
    "\n",
    "# Get the accuracy score for the neural network\n",
    "acc = regr.score(X_test2_scaled, y_test2)\n",
    "print('Accuracy:', str(round(acc * 100, 2)) + '%')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.8 64-bit (microsoft store)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "bbdc6700f60de612a9776dd7a4bd392f8898b0d00245e53755bddb2281dd5e23"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

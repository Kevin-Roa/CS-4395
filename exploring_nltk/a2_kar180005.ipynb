{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "(1 & 2)\n",
        "Import NLTK and relevant libraries"
      ],
      "metadata": {
        "id": "i3jTwXOL-fdS"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_DsJOD200_oD"
      },
      "outputs": [],
      "source": [
        "import nltk\n",
        "nltk.download('stopwords')\n",
        "nltk.download('wordnet')\n",
        "nltk.download('punkt')\n",
        "nltk.download('omw-1.4')\n",
        "nltk.download('book')\n",
        "\n",
        "from nltk.book import *"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "(3)\n",
        "Extract first 20 tokens from text1\n",
        "\n",
        "* Text1 is a _Text_ object which contains various useful methods for text analysis.\n",
        "* The tokens object contains a list of tokens for each book. The tokens appear to be words and individual symbols\n"
      ],
      "metadata": {
        "id": "Ckq-QGz--fBV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(text1.tokens[:20])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hfTes5jR-1a4",
        "outputId": "87e1e4c2-882d-4472-9289-ee2480aa0a3f"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['[', 'Moby', 'Dick', 'by', 'Herman', 'Melville', '1851', ']', 'ETYMOLOGY', '.', '(', 'Supplied', 'by', 'a', 'Late', 'Consumptive', 'Usher', 'to', 'a', 'Grammar']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "(4)\n",
        "Utilize the concordance method of the Text object to print the concordance for 'sea'"
      ],
      "metadata": {
        "id": "e8uCoxksHEG6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "text1.concordance('sea', lines=5)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "V3JtjEI9ABrI",
        "outputId": "54aeff5d-0f75-4e42-8cda-b8b3d620881f"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Displaying 5 of 455 matches:\n",
            " shall slay the dragon that is in the sea .\" -- ISAIAH \" And what thing soever \n",
            " S PLUTARCH ' S MORALS . \" The Indian Sea breedeth the most and the biggest fis\n",
            "cely had we proceeded two days on the sea , when about sunrise a great many Wha\n",
            "many Whales and other monsters of the sea , appeared . Among the former , one w\n",
            " waves on all sides , and beating the sea before him into a foam .\" -- TOOKE ' \n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "(5)\n",
        "The count method in the nltk Text object is functionally equivalent to the default python list count implementation. Looking into the actual implementation, the NLTK method simply calls the default list count method and passes the list of tokens."
      ],
      "metadata": {
        "id": "kdi8usSCCEjM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(len(text1))\n",
        "print(len(list(text1)))\n",
        "\n",
        "print(text1.count('sea'))\n",
        "print(list(text1).count('sea'))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tW_2llnyAPb6",
        "outputId": "d7951c0d-edee-4826-a486-c5cb521b6bef"
      },
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "260819\n",
            "260819\n",
            "433\n",
            "433\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "(6)\n",
        "Tokenize a piece of text by words using the NLTK word_tokenize method\n",
        "\n",
        "Source: [Bee Movie Script](https://gist.github.com/MattIPv4/045239bc27b16b2bcf7a3a9a4648c08a)"
      ],
      "metadata": {
        "id": "LXqjkg1YGxk5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "raw_text = \"\"\"According to all known laws of aviation, there is no way a bee should be able to fly.\n",
        "Its wings are too small to get its fat little body off the ground.\n",
        "The bee, of course, flies anyway because bees don't care what humans think is impossible.\n",
        "Yellow, black. Yellow, black. Yellow, black. Yellow, black.\n",
        "Ooh, black and yellow!\n",
        "Let's shake it up a little.\"\"\"\n",
        "\n",
        "tokens = nltk.word_tokenize(raw_text)\n",
        "print(tokens[:10])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RbIspgwoFyyR",
        "outputId": "e4fc60e5-2c17-49bb-a6f5-0b2cefe9ffde"
      },
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['According', 'to', 'all', 'known', 'laws', 'of', 'aviation', ',', 'there', 'is']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "(7) Tokenize a piece of text by sentences using the NLTK sent_tokenize method"
      ],
      "metadata": {
        "id": "5JnTrdekICef"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "sentences = nltk.sent_tokenize(raw_text)\n",
        "print(sentences)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hSCMJ-UKICuV",
        "outputId": "8e80c176-ffed-4220-871c-a076fcafe53a"
      },
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['According to all known laws of aviation, there is no way a bee should be able to fly.', 'Its wings are too small to get its fat little body off the ground.', \"The bee, of course, flies anyway because bees don't care what humans think is impossible.\", 'Yellow, black.', 'Yellow, black.', 'Yellow, black.', 'Yellow, black.', 'Ooh, black and yellow!', \"Let's shake it up a little.\"]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "(8) Stem a piece of text using the NLTK PorterStemmer() object"
      ],
      "metadata": {
        "id": "Sy-1rYqRJIbm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "stemmer = nltk.stem.PorterStemmer()\n",
        "print([stemmer.stem(t) for t in tokens])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "18-2NnZgIw4-",
        "outputId": "943b67d3-7b70-4bce-8740-388a30a574dd"
      },
      "execution_count": 38,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['accord', 'to', 'all', 'known', 'law', 'of', 'aviat', ',', 'there', 'is', 'no', 'way', 'a', 'bee', 'should', 'be', 'abl', 'to', 'fli', '.', 'it', 'wing', 'are', 'too', 'small', 'to', 'get', 'it', 'fat', 'littl', 'bodi', 'off', 'the', 'ground', '.', 'the', 'bee', ',', 'of', 'cours', ',', 'fli', 'anyway', 'becaus', 'bee', 'do', \"n't\", 'care', 'what', 'human', 'think', 'is', 'imposs', '.', 'yellow', ',', 'black', '.', 'yellow', ',', 'black', '.', 'yellow', ',', 'black', '.', 'yellow', ',', 'black', '.', 'ooh', ',', 'black', 'and', 'yellow', '!', 'let', \"'s\", 'shake', 'it', 'up', 'a', 'littl', '.']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "(9) Lemmatize a piece of text using the NLTK WordNetLemmatizer() object. \n",
        "The stemmer and lemmatizer are similar, yet have some minor differences in the results\n",
        "Some of the differences include:\n",
        "\n",
        "* accord vs According\n",
        "* aviat vs aviation\n",
        "* abl vs able\n",
        "* fli vs fly\n",
        "* it vs Its\n",
        "* littl vs little\n",
        "* bodi vs body\n",
        "\n",
        "There are many more examples of where they differ. \n",
        "\n"
      ],
      "metadata": {
        "id": "fzIAeRm_JRAE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "lemmer = nltk.stem.WordNetLemmatizer()\n",
        "print([lemmer.lemmatize(t) for t in tokens])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rnpUDW-aJRQ_",
        "outputId": "357dd9c4-7221-4f84-b11a-c1a56fb060c3"
      },
      "execution_count": 39,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['According', 'to', 'all', 'known', 'law', 'of', 'aviation', ',', 'there', 'is', 'no', 'way', 'a', 'bee', 'should', 'be', 'able', 'to', 'fly', '.', 'Its', 'wing', 'are', 'too', 'small', 'to', 'get', 'it', 'fat', 'little', 'body', 'off', 'the', 'ground', '.', 'The', 'bee', ',', 'of', 'course', ',', 'fly', 'anyway', 'because', 'bee', 'do', \"n't\", 'care', 'what', 'human', 'think', 'is', 'impossible', '.', 'Yellow', ',', 'black', '.', 'Yellow', ',', 'black', '.', 'Yellow', ',', 'black', '.', 'Yellow', ',', 'black', '.', 'Ooh', ',', 'black', 'and', 'yellow', '!', 'Let', \"'s\", 'shake', 'it', 'up', 'a', 'little', '.']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "(10)\n",
        "The NLTK library appears to provide some useful functionality for dealing with text analysis. The word and sentence tokenizers are convinient for splitting a given piece of text into the desired format. The stemmer and lemmatizer are useful for converting a set of tokens into a potentially more useful format while additionally decreasing the overall size of the data. \n",
        "\n",
        "Overall, the NLTK library seems to be well made. I do, however, wish that the documentation was more fleshed out in terms of object attributes/data types/implementation/information. It appears that the only provide minor commentary with the object source code which makes it somewhat tenious to learn to use. Though, I realize that I only looked at a very small portion of the library and am nowhere near utilizing it to its full potential. \n",
        "\n",
        "In the future I would like to look into text/speech sentiment analysis. I've seen other projects on sentiment analysis and it seems really interesting. NLTK would also be useful for data mining and deriving the most useful information from websites. "
      ],
      "metadata": {
        "id": "PGEHnEPUKtMl"
      }
    }
  ]
}